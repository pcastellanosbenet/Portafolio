### ¿ Qué son?  
Es una técnica que permite a un modelo ponderar(dar más importancia) a ciertas palabras o partes del texto al procesar la información en lugar de tratar toda la entrada por igual.  

### ¿Por qué son importante ?
Los mecanismos de atención som de gran importancia porque permiten a los modelos entender las relaciones entre las palabras aún y cuando estas estén lejos entre sí en una oración. Resuelve lo  
Además tienen una gran aplicaión en muchos modelos como Transformers, que son la base de modelos como los GPTs, BERT, etc. Esto introdujo  un cambio profundo en cómo se procesaban las secuencias en NLP reemplazando en gran medida a modelos como las RNNs y LSTM.
