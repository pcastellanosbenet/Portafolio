### ¿ Qué son los mecanismos de atención ?  
Es una técnica que permite a un modelo ponderar(dar más importancia) a ciertas palabras o partes del texto al procesar la información, en lugar de tratar toda la entrada por igual.  

### ¿Por qué son importante ?
Los mecanismos de atención son de gran importancia porque permiten a los modelos entender las relaciones entre las palabras, aún y cuando estas, están lejos entre sí en una oración. Además tienen una gran aplicaión en muchos modelos como los Transformers, que son la base de otros modelos como los GPTs y BERT ampliamente utilizados actualmente. Esto introdujo  un cambio profundo en cómo se procesaban las secuencias en NLP reemplazando en gran medida a modelos como las RNNs y LSTM.
